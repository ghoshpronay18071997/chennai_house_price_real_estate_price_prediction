# -*- coding: utf-8 -*-
"""chennai_house_price_prediction_final_ipython_notebook_for_github_project_gallery_insaid.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15gfBJ9NTYi51RPMMZzKU6VgGnQqLlmdr

<p align="center"><img src="https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true" width="260" height="110" /></p>

----
# **Table of Contents**
----

**1.** [**Problem Statement**](#section1)<br>

**2.** [**Installing and Importing Packages**](#section2)<br>
  - **2.1** [**Description of the Dataset**](#section301)<br>
  - **2.2** [**Upgrading Libraries**](#section302)<br>
  - **2.2** [**Importing Libraries**](#section302)<br>

**3.** [**Loading Data**](#section3)<br>

**4.** [**Data Acquistititon and Description**](#section4)<br>
  - **4.1** [**Description of the Dataset**](#section401)<br>
  - **4.2** [**Data Cleaning**](#section40201)<br>

**5.** [**Data Pre-Processing**](#section6)<br>
  - **5.1** [**Pre-Profiling Report**](#section401)<br>
  - **5.2** [**Data Information**](#section40201)<br>

**6.** [**Exploratory Data Analysis**](#section7)<br>
  - **6.1** [**Univariate Analysis**](#section401)<br>
  - **6.2** [**Bivariate Analysis**](#section40201)<br>
  
**7.** [**Data Postprocessing**](#section8)
  - **7.1** [**Encoding Categorical Variables**](#section401)<br>
  - **7.2** [**Feature Engineering**](#section40201)<br>
  - **7.3** [**Separating Train and Test Data**](#section40201)<br>

**8.** [**Modelling**](#section8)
  - **8.1** [**Defining Baseline Models**](#section401)<br>
  - **8.2** [**Hyperparameter Tuning**](#section40201)<br>

**9.** [**Test Set**](#section8)

**10.** [**Conclusion**](#section8)

---
# **1. Problem Statement:-**
---

- **ChennaiEstate** is a **real estate firm** based in **Chennai** that is involved in the property business for the past 5 years.

- Since, they are in the business for so long, they have enough data of all the real estate transactions in the city.

- They decided to venture into Analytics and have now started a division called **Chennai Estate Analytics** to give consumers as much information as possible about housings and the real estate market in Chennai.

-  A home is often the largest and most expensive purchase a person makes in his or her lifetime. Ensuring real-estate owners have a
trusted way to monitor the asset is incredibly important.

-  Hence, they have hired you as a consultant to help them give insights and develop a model to accurately predict real estate prices.

- Based on the **train** dataset, you will need to develop a model that accurately predicts the real estate price in **Chennai**.

<center><img src = "https://therealdeal.com/national/wp-content/uploads/2021/03/CoreLogic-Home-Price-Reports-Highest-Growth-Since-2013.gif"></center>

### **Scenario**

- You are given a dataset consisting of **required details** 

- Your task is to build a **regression model** using the dataset

- Because there was **no machine learning model for this problem** in the company, you donâ€™t have a quantifiable win condition. You need to build the best possible model

<a name=section2></a>

---
# **2. Importing Libraries**
---
"""

import sys                                                       # Importing System
!{sys.executable} -m pip install -U pandas-profiling[notebook]   # Installing pandas profiling
!jupyter nbextension enable --py widgetsnbextension              # enabling python notebook extention

!pip install pandas-profiling -q --upgrade                          # Installing pandas profiling  
!pip install catboost -q                                            # Intalling Catboost regressor  
!pip install xgboost -q                                             # Intalling XGboost regressor

"""<a name = Section22></a>
### **2.2 Upgrading Libraries**

- **After upgrading** the libraries, you need to **restart the runtime** to make the libraries in sync. 

- Make sure not to execute the cell above (2.1) and below (2.2) again after restarting the runtime.
"""

!pip install pandas-profiling -q --upgrade                          # upgrading pandas profiling

"""<a name = Section23></a>
### **2.3 Importing Libraries**
- After the **installation** and **upgrading of all the libraries** we will now import the necessary libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
#-------------------------------------------------------------------------------------------------------------------------------

import matplotlib.pyplot as plt                                     # Importing pyplot for visualization
# %matplotlib inline

#----------------------------------------------------------------------------------------------
import seaborn as sns
#-------------------------------------------------------------------------------------------------------------------------------
import pandas as pd                                                 # Importing for panel data analysis
from pandas_profiling import ProfileReport                          # Importing Pandas Profiling (To generate Univariate Analysis)
pd.set_option('display.max_columns', None)                          # Unfolding hidden features if the cardinality is high
pd.set_option('display.max_colwidth', None)                         # Unfolding the max feature width for better clearity
pd.set_option('display.max_rows', None)                             # Unfolding hidden data points if the cardinality is high
pd.set_option('mode.chained_assignment', None)                      # Removing restriction over chained assignments operations
pd.set_option('display.float_format', lambda x: '%.5f' % x)         # To suppress scientific notation over exponential values
#-------------------------------------------------------------------------------------------------------------------------------
import numpy as np                                                  # Importing package numpys (For Numerical Python)
from datetime import datetime as dt                                 # For datetime funcationality
import warnings                                                     # Importing warning to disable runtime warnings
warnings.filterwarnings("ignore")                                   # Warnings will appear only once
#-------------------------------------------------------------------------------------------------------------------------------
from sklearn import metrics                                         # Calling the metrics for calculating performence metrics
from sklearn import preprocessing                                   # Calling preprocessing for preprocessing of data
from sklearn.preprocessing import StandardScaler                    # Calling standardscaler for standerdization
#-------------------------------------------------------------------------------------------------------------------------------
from sklearn.svm import SVR                                         # Calling Support Vector Regressor for modelling
from sklearn.decomposition import PCA                               # Calling PCA for dimentionality reduction
from sklearn.metrics import make_scorer                             # Calling make_scorer for calculating score
from sklearn.metrics import mean_squared_error                      # Calling sean_squared_error for calculating mean squared error
import zipfile                                                      # Importing zipfile
#-------------------------------------------------------------------------------------------------------------------------------
from scipy import stats                                             # Importing stats from scipy
from scipy.stats import norm                                        # Importing norm
#-------------------------------------------------------------------------------------------------------------------------------
from sklearn.preprocessing import LabelEncoder, MinMaxScaler        # Importing Encoders
from sklearn.linear_model import LinearRegression, Ridge, Lasso     # Importing Linear Regressors
from sklearn.neighbors import KNeighborsRegressor                   # Importing KNN
from sklearn.ensemble import RandomForestRegressor                  # Importing Random Forest Regressor
from sklearn.ensemble import BaggingRegressor                       # Importing Bagging Regressor
from sklearn.ensemble import GradientBoostingRegressor              # Importing GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor                      # Importing DecissionTreeeRegressor
from catboost import CatBoostRegressor                              # Importing CatBoostRegressor
from xgboost import XGBRegressor                                    # Importing XGBoost Regressor
#-------------------------------------------------------------------------------------------------------------------------------
from sklearn.model_selection import train_test_split                # Calling train_test_split for splitinng the dataset 
from sklearn.model_selection import RandomizedSearchCV              # Calling RandomizedSearchCV for tuning the model
from sklearn.model_selection import cross_val_score                 # Importing cross_val_score  
from sklearn.model_selection import GridSearchCV                    # Importing GridsearchCV
#-------------------------------------------------------------------------------------------------------------------------------
from sklearn.metrics import mean_squared_error                      # Importing MSE
from sklearn.metrics import r2_score                                # Importing R Squared
from sklearn.metrics import mean_absolute_error                     # Importing MAE
#-------------------------------------------------------------------------------------------------------------------------------
import time                                                         # Importing time
import re                                                           # Importing RegEx  
import plotly.express as ex                                         # Importing Plotly Express for Dynamic Plotting
import plotly.graph_objs as go                                      # Importing Plotly graphs for Dynamic Plotting
import plotly.offline as pyo                                        # Importing offline Express for Dynamic Plotting
from plotly.subplots import make_subplots                           # Importing Plotly Subplots to plot Dynamic subplots
import plotly.figure_factory as ff                                  # Calling the figure factory to create unique chart types

"""----
<a id=section3></a>
# **3. Loading Data**
----

- In this step we will be **Loading the dataset**
"""

data = pd.read_csv("/content/chennai_house_price_prediction.csv")

data.head()

data.shape

"""----
<a id=section3></a>
# **4. Data Acquistion and Description**
----

### **4.1 Description of the Dataset**
- We have **71092 samples**  and for each of sample **19 different** properties are recorded.
"""

data.describe()

"""**Observation:**

- The emean of the **SALES_PRICE** is found to be **10894909.63919** which is deviating from the std.

<a name = Section42></a>
### **4.2 Data Information**

- In this section we will see the **information about the types of features**.
"""

data.info()

"""### **Observation:**

- There are total **13 numerical data-type and 8 object type data files** recorded.

<a name = Section5></a>

---
# **5. Data Pre-Processing**
---

<a name = Section51></a>
### **5.1 Pre Profiling Report**

- For **quick analysis** pandas profiling is very handy.

- Generates profile reports from a pandas DataFrame.

- For each column **statistics** are presented in an interactive HTML report.
"""

profile = ProfileReport(df=data)
profile.to_file(output_file='Pre Profiling Report.html')
print('Accomplished!')

"""<a name = Section5></a>
### **5.2 Data Cleaning**

- In this section, we will clean out our data based on the information retrieved from the previous observations.

- Hence, we will have to perform thr following subtasks
  - Checking for missing values and manipulating them
  - Checking the datatype
  - Spelling Correction
"""

# Checking for missing values and manipulating them
data.isnull().sum()

#Checking the datatype
data.dtypes

# Treating the missing values:-
data["QS_OVERALL"] = data["QS_OVERALL"].fillna(data["QS_OVERALL"].mean())
data["N_BEDROOM"] = data["N_BEDROOM"].fillna(data["N_BEDROOM"].mean())
data["N_BATHROOM"] = data["N_BATHROOM"].fillna(data["N_BATHROOM"].mean())

data.isnull().sum()

# Checking Value Counts:- 
for i in data.columns:
    print("************************************")
    print("The Value Count for " + i + " is :-" )
    print(data[i].value_counts())
    print("************************************")

# Dropping PRT_ID:- 
data = data.drop(["PRT_ID"],axis=1)

# Replacing "AREA" with the respective values:
data["AREA"].replace(to_replace = ["Chrompt","Chormpet","Chrmpet"], value ="Chrompet",inplace=True)

data["AREA"].replace(to_replace=["Karapakam","KKNagar","Velchery","Ana Nagar","Ann Nagar","Adyr","TNagar"], 
                     value=["Karapakkam","KK Nagar","Velachery","Anna Nagar","Anna Nagar","Adyar","T Nagar"]  
                    ,inplace=True)

data["AREA"].value_counts()

pd.get_dummies(data["SALE_COND"])
data["SALE_COND"].replace(to_replace=["Adj Land","Partiall","PartiaLl","Ab Normal"], 
                     value=["AdjLand","Partial","Partial","AbNormal"]  
                    ,inplace=True)

data["SALE_COND"].value_counts()

data["SALE_COND"].value_counts()

data.head()

data["PARK_FACIL"].value_counts()

data["PARK_FACIL"].replace(to_replace=["Noo"],value = ["No"], inplace = True)

data["PARK_FACIL"].value_counts()

data["BUILDTYPE"].value_counts()

data["BUILDTYPE"].replace(to_replace=["Comercial","Other"],value = ["Commercial","Others"], inplace = True)

data["BUILDTYPE"].value_counts()

data["UTILITY_AVAIL"].value_counts()

data["UTILITY_AVAIL"].replace(to_replace=["All Pub"],value = ["AllPub"], inplace = True)

data["UTILITY_AVAIL"].value_counts()

data["STREET"].value_counts()

data["STREET"].replace(to_replace = ["Pavd","NoAccess"], value=["Paved","No Access"], inplace = True)

data["STREET"].value_counts()

data["MZZONE"].value_counts()

datatype = data.dtypes

datatype

"""<a name = Section5></a>

---
# **6. Exploratory Data Analysis**
---

### **6.1 Univariate Analysis**

- In this section we will see what information can be derived from each individual feature of the dataset.

#### **Question:** What insights can be drawn from the categorical features ?
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

for i in data.columns:
    if datatype[i] == 'object':
        plt.figure(figsize = (10,5))
        sns.barplot(x = data[i].value_counts().values,
                    y = data[i].value_counts().index,
                    orient = 'h')
        plt.ylabel(i)

"""**Observations:**

- The freq of **RL** is maximum for **MZZONE**

- **Paved** type streets has been reported maximum number of times.

- **UTILITY_AVAIL** has **AllPub** recorded the maximum number of times.

#### **Question:** What insights can be drawn from the numerical features ?
"""

for i in data.columns:
    if datatype[i] != 'object':
        plt.figure(figsize = (10,5))
        data[i].plot.hist(bins=50)
        plt.xlabel(i)
        plt.grid()

"""### **6.2 Univariate Analysis**

- In this section we will see what information can be derived from more than one features at a time. 
"""

# data_bivariate= data.copy()

"""#### **Question:** What is the relation in between **BUILDTYPE** and **PARK_FACIL** w.r.t average **SALES_PRICE**?"""

import seaborn as sns
sns.set()
temp = data.groupby(['BUILDTYPE', 'PARK_FACIL']).SALES_PRICE.median()
temp.plot(kind = 'bar', stacked = True)

"""#### **Question:** What is the deviation in **PARK_FACIL** w.r.t **SALES_PRICE**?"""

temp = data.groupby(['PARK_FACIL']).SALES_PRICE.mean()
temp_index = temp.index
temp_values = temp.values 
sns.barplot(temp_values,
            temp_index,
            orient = 'h')
plt.grid()

"""#### **Question:** How **SALE_PRICE** effects the **distance from main road**?"""

plt.scatter(data['DIST_MAINROAD'],data['SALES_PRICE'])

"""----
<a id=section6></a>
# **7. Data Postprocecssing**
----

<a name = Section71></a>
### **7.1 Data encoding**

- We will use **encode** the necessary features
"""

# OneHot Encode the Catagorical Variables
a,b,c,d,e,f = pd.get_dummies(data["AREA"]),pd.get_dummies(data["PARK_FACIL"]),pd.get_dummies(data["BUILDTYPE"]),pd.get_dummies(data["UTILITY_AVAIL"]),pd.get_dummies(data["STREET"]),pd.get_dummies(data["MZZONE"])

y= pd.get_dummies(data["SALE_COND"])

a.head()

b.head()

c.head()

d.head()

e.head()

f.head()

"""<a name = Section71></a>
### **7.2 Feature genaration**

- In this step we will use **genarate new features**
"""

for i in range(len(a.columns)):
    data.insert(0+i,a.columns[i],a[a.columns[i]])
for i in range(len(b.columns)):
    data.insert(11+i,b.columns[i],b[b.columns[i]])
for i in range(len(c.columns)):
    data.insert(12+i,c.columns[i],c[c.columns[i]])
for i in range(len(d.columns)):
    data.insert(13+i,d.columns[i],d[d.columns[i]])
for i in range(len(e.columns)):
    data.insert(14+i,e.columns[i],e[e.columns[i]])
for i in range(len(f.columns)):
    data.insert(15+i,f.columns[i],f[f.columns[i]])

for i in range(len(y.columns)):
    data.insert(7+i,y.columns[i],y[y.columns[i]])

"""<a name = Section71></a>
### **7.2 Feature Dropping**

- In this step we will use **genarate new features**
"""

# Dropping unnecessary features
data.drop(['AREA','SALE_COND','PARK_FACIL', 'BUILDTYPE', 'UTILITY_AVAIL', 'STREET', 'MZZONE'], axis=1 , inplace= True)

data.head()

"""<a name = Section72></a>
### **7.4 Feature Extraction**

- We will seperate the dataframe into train and validation sets.

- We will **seperate** the train set into X (independent) and y (dependent) dataframes.

- Finally, we will apply **train-test split** on the scaled data.
"""

x= data.drop(["SALES_PRICE"],axis=1)
y= data["SALES_PRICE"]

"""<a id=section7></a>

---
# **8. Model Building**
---

### **8.1 Defining Baseline Models**
- In this section we will define all of the **best possible basine models** and train using the **training data**
"""

# Defining the scores list
model_scores = []

# Defining a list of useful regressors
regressors = [RandomForestRegressor(random_state=42), 
              KNeighborsRegressor(),
              GradientBoostingRegressor(random_state=42),
              CatBoostRegressor(random_state=42),
              XGBRegressor(random_state=42),
              BaggingRegressor(random_state=42)]

"""### **8.2 Splitting training and Testing Data**
- In this section we will make the **train** and the **test** data.
"""

X_train, X_test, y_train, y_test = train_test_split(x, y,
                                                    test_size = 0.25,
                                                    random_state = 42)

for reg in regressors:
  # Extracting model name
  model_name = type(reg).__name__

  # Fit the model on train data
  reg.fit(X_train, y_train)

  # Make predictions using train data
  y_train_pred = reg.predict(X_train)

  # Make predictions using test data
  y_pred = reg.predict(X_test)

  # Calculate train rmse of the model
  reg_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))

  # Calculate test rmse of the model
  reg_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
  
  # Calculating train R2 Score
  reg_train_r2 = r2_score(y_train, y_train_pred)

  # Calculating test R2 Score
  reg_test_r2 = r2_score(y_test, y_pred)

  # Display the accuracy of the model
  print('Performance Metrics for', model_name, ':')
  #print('[RMSE-Score Train]:', reg_train_rmse)
  #print('[RMSE-Score Test]:', reg_test_rmse)
  print('[R2-Score Train]:', reg_train_r2)
  print('[R2-Score Test]:', reg_test_r2)

  model_scores.append((model_name,
                       reg_train_r2,
                       reg_test_r2))

  print('--------------------\n')

models = pd.DataFrame(data=model_scores, columns=['Model', '[R2-Score Train]', '[R2-Score Test]'])
models

# Plotting the RMSE scores for each model
fig = plt.figure(figsize=(15,7))
sns.barplot((models['[R2-Score Test]']), models['Model'], palette='rocket')
plt.grid(b=True)

"""### **8.2 Hyperpameter tuning**

- In this section we will perform **hyperparameter tuning** and check how our models perform post tuning.

- We will be using **Random Search** in order to find the best values.

- We will consider **CatBoost Regressor** and **XGB Regresosor** as they have given best results
"""

# Creating a parameter grid for CatBoost Regressor
param_grid_cat  = {'iterations': [100, 150, 200],
                   'learning_rate': [0.03, 0.1],
                   'depth': [2, 4, 6, 8],
                   'l2_leaf_reg': [0.2, 0.5, 1, 3]}

cbr = CatBoostRegressor(random_state=42)
gscv = RandomizedSearchCV(estimator = cbr,
                          param_distributions = param_grid_cat,
                          scoring ='r2',
                          cv = 5)
gscv.fit(X_train, y_train)

# Printing metrics
print("[Hyperparameters]:", gscv.best_params_)
print("[Train Score]:", gscv.best_score_)
print("[Validation Score]:", r2_score(y_test,
                                      gscv.predict(X_test)))

"""<a id=section7></a>

---
# **9. Test Set**
---
- We will use the **CatBoost regressor** as gives the best **R2 Score** among the three tuned models.
"""

# Predicting with the best fit parameters
best_fit = gscv.best_estimator_
best_fit.fit(X_train, y_train)

# storing all best tvalues in to y_pred_tuned
y_pred_tuned = best_fit.predict(X_test)

from sklearn import metrics
print('R2 Score in Test data is : ',r2_score(y_test, y_pred_tuned))

"""<a id=section7></a>

---
# **10. Making Pickle File**
---
- We can see that our model performs **really good** in **unseen data**.
- Hence, we can say that our model is **Ready to Deploy**
- For this we will have to import **pickle**
- After that we need to form a **.pkl file**
"""

import pickle
# open a file, where you ant to store the data
file = open('chennai_house.pkl', 'wb')

# dump information to that file
pickle.dump(best_fit, file)

"""<a id=section7></a>

---
# **10. Conclusion**
---

- In this case study the given data was analysed and on top of that a **regression model** was built.

- The model chosen for this case study was a **Catboost regressor** as it was retruning the least overfitting and best r2 score on **unseen data**

- The **r2 score** genarated in unseen data was **0.99** which means that the modedl performs really good and is generalizing well on unseen data.

- After modelling we dumped oue model into a pickle file and we can conclude that now our model is **ready to deploy**
"""